{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e1e50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, Markdown\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66a49d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e67f35",
   "metadata": {},
   "source": [
    "# Step 1: Explore the data\n",
    "\n",
    "Explore the data in full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceeaa9c",
   "metadata": {},
   "source": [
    "## 1. Metdata csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d4c7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    _id     duration            disposition  \\\n",
      "0  1558137f-4ab5-409e-8159-109c2e9358d5  3927.771375    No Pay - Bankruptcy   \n",
      "1  05c54daf-9bca-4bd7-b833-b3ba8c1e06e0   146.520000  No Pay - Cant pay now   \n",
      "2  84fb4fec-b36f-43bc-a4ef-a27ba322c146   749.635875       No Pay - Dispute   \n",
      "3  2133927a-0d11-4d54-98d6-cd21c71417f0   448.104375       No Pay - Dispute   \n",
      "4  048518c6-c17c-4524-8ff3-6a0b2735863c   108.432000       No Pay - Dispute   \n",
      "\n",
      "       type  \n",
      "0  negative  \n",
      "1  negative  \n",
      "2  negative  \n",
      "3  negative  \n",
      "4  negative  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   _id          2000 non-null   object \n",
      " 1   duration     2000 non-null   float64\n",
      " 2   disposition  2000 non-null   object \n",
      " 3   type         2000 non-null   object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 62.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_call_metadata = pd.read_csv('/Users/sohammistri/Documents/Prodigal-Take-Home/sentiment-take-home/call_metadata.csv')\n",
    "print(df_call_metadata.head())\n",
    "print(df_call_metadata.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9debc07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "negative    1000\n",
      "positive    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_call_metadata['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e73298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disposition\n",
      "No Pay - Dispute                    439\n",
      "Promise - Payment in full           313\n",
      "No Pay - Not right now              296\n",
      "Promise - Payment plan              256\n",
      "No Pay - Cant pay now               182\n",
      "Promise - Settlement in full        163\n",
      "Promise - One time payment          104\n",
      "Payment Plan Modification           101\n",
      "Promise - Settlement in payments     63\n",
      "No Pay - Can't pay now               42\n",
      "No Pay - Bankruptcy                  30\n",
      "No Pay - Cancel payment plan         11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_call_metadata['disposition'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a12f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                              negative  positive\n",
      "disposition                                         \n",
      "No Pay - Bankruptcy                     30         0\n",
      "No Pay - Can't pay now                  42         0\n",
      "No Pay - Cancel payment plan            11         0\n",
      "No Pay - Cant pay now                  182         0\n",
      "No Pay - Dispute                       439         0\n",
      "No Pay - Not right now                 296         0\n",
      "Payment Plan Modification                0       101\n",
      "Promise - One time payment               0       104\n",
      "Promise - Payment in full                0       313\n",
      "Promise - Payment plan                   0       256\n",
      "Promise - Settlement in full             0       163\n",
      "Promise - Settlement in payments         0        63\n"
     ]
    }
   ],
   "source": [
    "disposition_type_counts = df_call_metadata.groupby('disposition')['type'].value_counts().unstack(fill_value=0)\n",
    "print(disposition_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27db20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive disposition types: {'Promise - Settlement in full', 'Promise - One time payment', 'Promise - Payment plan', 'Promise - Settlement in payments', 'Promise - Payment in full', 'Payment Plan Modification'}\n",
      "Negative disposition types: {'No Pay - Not right now', 'No Pay - Cant pay now', 'No Pay - Bankruptcy', 'No Pay - Dispute', 'No Pay - Cancel payment plan', \"No Pay - Can't pay now\"}\n"
     ]
    }
   ],
   "source": [
    "positive_dispositions = set(disposition_type_counts[disposition_type_counts['positive'] > 0].index.tolist())\n",
    "negative_dispositions = set(disposition_type_counts[disposition_type_counts['negative'] > 0].index.tolist())\n",
    "\n",
    "print(\"Positive disposition types:\", positive_dispositions)\n",
    "print(\"Negative disposition types:\", negative_dispositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "046788cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_dispositions), len(negative_dispositions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfa378",
   "metadata": {},
   "source": [
    "### Split dataset into train-val-test\n",
    "\n",
    "We do 80-10-10 train-val-test split, ensuring there is no class imbalance in either split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c10097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1600, 4), Positive: 800, Negative: 800\n",
      "Val shape: (200, 4), Positive: 100, Negative: 100\n",
      "Test shape: (200, 4), Positive: 100, Negative: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate positive and negative samples\n",
    "df_positive = df_call_metadata[df_call_metadata['type'] == 'positive']\n",
    "df_negative = df_call_metadata[df_call_metadata['type'] == 'negative']\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "def split_df(df, train_size, val_size, test_size):\n",
    "    df_train, df_temp = train_test_split(df, train_size=train_size, random_state=42, shuffle=True)\n",
    "    relative_val_size = val_size / (val_size + test_size)\n",
    "    df_val, df_test = train_test_split(df_temp, train_size=relative_val_size, random_state=42, shuffle=True)\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "# Split positive and negative samples\n",
    "pos_train, pos_val, pos_test = split_df(df_positive, train_size, val_size, test_size)\n",
    "neg_train, neg_val, neg_test = split_df(df_negative, train_size, val_size, test_size)\n",
    "\n",
    "# Concatenate to get final splits with equal positive and negative samples\n",
    "train_df = pd.concat([pos_train, neg_train]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df = pd.concat([pos_val, neg_val]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat([pos_test, neg_test]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Positive: {(train_df['type'] == 'positive').sum()}, Negative: {(train_df['type'] == 'negative').sum()}\")\n",
    "print(f\"Val shape: {val_df.shape}, Positive: {(val_df['type'] == 'positive').sum()}, Negative: {(val_df['type'] == 'negative').sum()}\")\n",
    "print(f\"Test shape: {test_df.shape}, Positive: {(test_df['type'] == 'positive').sum()}, Negative: {(test_df['type'] == 'negative').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ecadfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600 entries, 0 to 1599\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   _id          1600 non-null   object \n",
      " 1   duration     1600 non-null   float64\n",
      " 2   disposition  1600 non-null   object \n",
      " 3   type         1600 non-null   object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 50.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a3f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('/Users/sohammistri/Documents/Prodigal-Take-Home/sentiment-take-home/train_df.csv', index=False)\n",
    "val_df.to_csv('/Users/sohammistri/Documents/Prodigal-Take-Home/sentiment-take-home/val_df.csv', index=False)\n",
    "test_df.to_csv('/Users/sohammistri/Documents/Prodigal-Take-Home/sentiment-take-home/test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e40fd",
   "metadata": {},
   "source": [
    "## 2. Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62d7c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text files: 2000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "conversation_folder = '/Users/sohammistri/Documents/Prodigal-Take-Home/sentiment-take-home/conversations'\n",
    "txt_files = [f for f in os.listdir(conversation_folder) if f.endswith('.txt')]\n",
    "print(f\"Number of text files: {len(txt_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3d5f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '.txt' extension from txt_files to get basenames\n",
    "txt_basenames = set([os.path.splitext(f)[0] for f in txt_files])\n",
    "\n",
    "# Get set of _id values from df_call_metadata\n",
    "df_ids = set(df_call_metadata['_id'])\n",
    "\n",
    "assert txt_basenames == df_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6d57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_call_metadata['_id'].is_unique, \"Not all _id values are unique in df_call_metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bbd9b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 89cd6569-3d4e-4c45-84e8-c6941f8f624d.txt\n",
       "\n",
       "agent 00:06-00:16\n",
       "* * * this call is being recorded and may be monitored for quality assurance purposes and by continuing you're providing consent may i please have your first and last name\n",
       "\n",
       "borrower 00:19-00:20\n",
       "* *\n",
       "\n",
       "agent 00:21-00:24\n",
       "okay thank you and do you happen to have your agency XYZ account number\n",
       "\n",
       "borrower 00:25-00:26\n",
       "no i don't\n",
       "\n",
       "agent 00:27-00:30\n",
       "okay no worries did you receive an email text or a letter from us\n",
       "\n",
       "borrower 00:31-00:33\n",
       "email text one letter sorry\n",
       "\n",
       "agent 00:34-00:37\n",
       "okay got it let me see if i can locate your account what is the year of your birthday please the year\n",
       "\n",
       "borrower 00:39-00:41\n",
       "* * * * *\n",
       "\n",
       "agent 00:42-00:48\n",
       "okay let me see here if you received the letter let me try to search it that way what is your address to where we send that letter to\n",
       "\n",
       "borrower 00:50-00:56\n",
       "* * * * * * * * * * * * *\n",
       "\n",
       "agent 00:57-01:22\n",
       "okay thank you okay got it i do see an account so very quickly agency XYZ is a debt collection agency this is an attempt to collect debt and any information obtained will be used for that purpose alright so i do see the account with lvnv funding llc originally with via inc and it has a balance of four hundred ninety dollars and nineteen cents did you wanna pay that in full today\n",
       "\n",
       "borrower 01:24-01:32\n",
       "i already sent you a check today certified you guys will be signing to get the cashier check it's coming so you should receive it by tomorrow\n",
       "\n",
       "agent 01:33-02:07\n",
       "alright i'll go ahead and make the note on the account and as soon as we receive it we'll send you an email confirmation of the account being closed okay perfect alrighty now a couple of things just to make sure we send it to the correct email we have * your last name * * * is that a good email twelve phone mills perfect so we'll send conns confirmation there and the phone number we have on file is it * * * * * * * * * * is that also accurate\n",
       "\n",
       "borrower 01:56-01:57\n",
       "yep yep\n",
       "\n",
       "borrower 02:07-02:09\n",
       "yep yep\n",
       "\n",
       "agent 02:09-02:12\n",
       "perfect alrighty for now was there anything else i can assist you with\n",
       "\n",
       "borrower 02:14-02:14\n",
       "nope\n",
       "\n",
       "agent 02:15-02:19\n",
       "perfect alright well i do thank you for calling agency XYZ and i hope you have a great rest of your day"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_txt = random.choice(txt_files)\n",
    "txt_path = os.path.join(conversation_folder, random_txt)\n",
    "with open(txt_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "display(Markdown(f\"### {random_txt}\\n\\n{content}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488beac",
   "metadata": {},
   "source": [
    "### Format every content better\n",
    "\n",
    "I want to create a format of the convo as `{\"borrower\": <text>, \"agent\": <text>, ...}` and save them in json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faf326aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conv_from_content(content):\n",
    "    lines = content.splitlines()\n",
    "    lines_fil = [line.strip() for line in lines if len(line.strip()) > 0]\n",
    "    convo = []\n",
    "    speaker = None\n",
    "    text = []\n",
    "    speaker_re = re.compile(r'^(agent|borrower)\\s+\\d{2}:\\d{2}-\\d{2}:\\d{2}$', re.IGNORECASE)\n",
    "    for line in lines_fil:\n",
    "        if speaker_re.match(line):\n",
    "            if speaker and text:\n",
    "                convo.append({\"speaker\": speaker, \"text\": \" \".join(text)})\n",
    "            speaker = line.split()[0].lower()\n",
    "            text = []\n",
    "        else:\n",
    "            text.append(line)\n",
    "    if speaker and text:\n",
    "        convo.append({\"speaker\": speaker, \"text\": \" \".join(text)})\n",
    "    return convo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f845a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt_file in txt_files:\n",
    "    txt_path = os.path.join(conversation_folder, txt_file)\n",
    "    with open(txt_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    formatted = format_conv_from_content(content)\n",
    "    json_path = os.path.splitext(txt_path)[0] + '.json'\n",
    "    with open(json_path, 'w') as jf:\n",
    "        json.dump(formatted, jf, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ded49",
   "metadata": {},
   "source": [
    "# Step 2: Remove Potential PII data\n",
    "\n",
    "For each conversation, flag potential PII data in the call logs. Mostly looked clean, but imp to do a sanity check on a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c89aee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_filter_prompt = \"\"\"\n",
    "# Context:\n",
    "\"You are an AI assistant tasked with identifying and flagging customer Personally Identifiable Information (PII) within a given text. Your role is to act as a data protection filter.\n",
    "\n",
    "You will be given a conversation between an AI agent and a customer as input. Here is how you should operate:\n",
    "1.  Analyze the following conversation content provided between an AI agent and a customer.\n",
    "2.  Your primary objective is to determine if any customer PII is present in the text. PII includes, but is not limited to:\n",
    "    *   Names (full names, first names, last names)\n",
    "    *   Email addresses\n",
    "    *   Phone numbers\n",
    "    *   Physical addresses (street, city, state, zip code)\n",
    "    *   Social Security numbers or other government-issued identification numbers\n",
    "    *   Credit card numbers or financial account information\n",
    "    *   Dates of birth\n",
    "    *   Login credentials (usernames, passwords)\n",
    "    *   Medical information\n",
    "    *   IP addresses or other unique device identifiers\n",
    "3. Crucially, you must **ignore any data that has been masked**. Masked data is information that has been partially or fully obscured. Common examples of masking techniques to ignore include:\n",
    "    - Redaction/Masking Out: Replacing parts of the data with characters like 'X', '*', or '#'. For instance, XXX-XX-1234 or john.doe@******.com.\n",
    "    - Substitution/Replacement: Replacing sensitive data with fictitious yet realistic-looking data. For example, a real name might be replaced with a randomly generated one.\n",
    "    - Tokenization/Encryption: Replacing data with a non-sensitive placeholder or an encrypted string.\n",
    "    - Nulling Out: Replacing sensitive information with null values or blanks.\n",
    "4. **Ignore Agent Names**: You must differentiate between the agent and the customer and ignore the agent's name. The agent's name may be stated explicitly (e.g., \"My name is Alex,\" \"This is SupportBot,\" or \"You're speaking with Sarah.\"). Your focus is exclusively on identifying the **customer's PII**.\n",
    "5.  After your analysis, you will provide a binary output:\n",
    "    *   Return **1** if you identify any potential PII in the conversation that is unmasked.\n",
    "    *   Return **0** if you do not find any PII in the conversation or **all** potential PII is masked out\n",
    "\n",
    "# Output Format:\n",
    "- Analysis of the conversation: <Detailed analysis of the conversation between the AI agent and customer wrt finding potential PII>\n",
    "- Identified PII: <List of potential customer PII data identified in the conversation. Return None if the input is customer PII free>\n",
    "- Score: <0 if there is no customer PII in the conversation or all customer PII is masked, 1 otherwise>\n",
    "\n",
    "IMPORTANT: Remember to always adhere to this output format for consistency\n",
    "\n",
    "# Input\n",
    "Input Content: {conv} \n",
    "\n",
    "# Output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cd5ae1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Analysis of the conversation:  \n",
       "Upon reviewing the conversation, I analyzed all exchanges between the agent and the borrower (customer). All possible PII fields such as names (\"miss *\", etc.), email addresses, and other sensitive data were indicated with masking (\"*,\" \"* *,\" etc.) and are therefore not to be considered as actual unmasked PII under the provided guidelines. Additionally, a statement regarding the customer's health status (\"i'm going through cancer right now i i'm i'm in radiation\") is mentioned, which is medical information and constitutes PII; however, it is not masked and thus counts as unmasked PII. No unmasked names, contact details, or financial/account information are present—the rest is either masked or relates to public company/agent information.\n",
       "\n",
       "- Identified PII: \n",
       "    - Medical information: The customer discloses undergoing cancer treatment and radiation (\"i'm going through cancer right now i i'm i'm in radiation i just got a radiation\").\n",
       "\n",
       "- Score: 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = deepcopy(pii_filter_prompt)\n",
    "random_txt = random.choice(txt_files)\n",
    "txt_path = os.path.join(conversation_folder, random_txt)\n",
    "txt_path = '/Users/sohammistri/Documents/Prodigal-Take-Home/sentiment-take-home/conversations/f8832c53-8b6c-4f73-bcce-b06c4f89b0f3.txt'\n",
    "with open(txt_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "p = p.format(conv=content)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=p\n",
    ")\n",
    "\n",
    "display(Markdown(response.output_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9827dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text):\n",
    "    analysis = None\n",
    "    pii = None\n",
    "    score = None\n",
    "\n",
    "    analysis_match = re.search(r'- Analysis of the conversation:\\s*(.*?)(?:- Identified PII:|$)', text, re.DOTALL)\n",
    "    pii_match = re.search(r'- Identified PII:\\s*(.*?)(?:- Score:|$)', text, re.DOTALL)\n",
    "    score_match = re.search(r'- Score:\\s*(\\d+)', text)\n",
    "\n",
    "    if analysis_match:\n",
    "        analysis = analysis_match.group(1).strip()\n",
    "    if pii_match:\n",
    "        pii = pii_match.group(1).strip()\n",
    "    if score_match:\n",
    "        score = int(score_match.group(1).strip())\n",
    "\n",
    "    return analysis, pii, score\n",
    "\n",
    "def get_pii_score(prompt, file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    try:\n",
    "        p = deepcopy(prompt)\n",
    "        p = p.format(conv=content)\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            input=p\n",
    "        )\n",
    "        response_out = response.output_text\n",
    "        analysis, pii, score = extract_sections(response_out)\n",
    "        result = {\n",
    "            \"file\": file_path,\n",
    "            \"analysis\": analysis,\n",
    "            \"pii\": pii,\n",
    "            \"score\": score\n",
    "        }\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47ad30c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_paths = [os.path.join(conversation_folder, fname) for fname in txt_files]\n",
    "len(txt_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df09a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 2000/2000 [09:12<00:00,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(get_pii_score, pii_filter_prompt, file_path) for file_path in txt_paths]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Files\"):\n",
    "        results.append(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7c1ff4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fil = [res for res in results if res is not None]\n",
    "results_df = pd.DataFrame(results_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d75234a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score\n",
      "0    1894\n",
      "1     106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(results_df['score'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84bee5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(0, len(results_df[results_df['score'] == 1]))\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0308ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The conversation between the agent and the customer involves a debt collection call. The customer is addressed by the name \"axel\" by the agent, which may indicate the customer's first name is unmasked and present. The agent also attempts to verify sensitive customer information: date of birth, mailing address, and email address; however, in all these instances, the information is masked with asterisks (\"* * * * *\" or similar formats). There are references to accounts and a transaction amount, but no unmasked account numbers, social security numbers, or other direct PII. The only potential PII that is not masked is the given name \"axel,\" which the agent uses to address the customer, and which the customer does not correct or dispute. All other potentially sensitive information (date of birth, address, email) is completely masked."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_df[results_df['score'] == 1].iloc[i][\"analysis\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d0e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
